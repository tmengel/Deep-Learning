{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c9150a5",
   "metadata": {
    "id": "3c9150a5"
   },
   "source": [
    "*__Project 3__*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c60a424",
   "metadata": {},
   "source": [
    "Project 3 involves building a face attribute classifier using the FairFace dataset which contains 86,744 training images and 10,954 validation images of faces. The images have been converted to grayscale and resized to 32x32. There are three attributes to classify: race, gender, and age. The project involves experimenting with different neural network architectures and measuring the performance of each on the classification tasks. There are 4 classification models and 1 generative model we are exploring. The performance of each network will be measured using accuracy and confusion matrices. The models are as follows:\n",
    "\n",
    "    * 1. Fully Connected Neural Network\n",
    "    * 2. Small Convolutional Neural Network\n",
    "    * 3. Custom Convolutional Neural Network\n",
    "    * 4. Multi-Task Convolutional Neural Network for two of the three attributes\n",
    "    * 5. Variational Autoencoder\n",
    "\n",
    "Each of the models will be explored in detail in thier respective sections below. The training for each model will be done on Google Colab and the results will be saved html files. The results will be discussed in the Results section. Each of these models will be trained on the task of classifying age and gender. These were chosen because gender has a binary output and age has a continuous output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IZrdx__XMKCz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZrdx__XMKCz",
    "outputId": "af12d8b0-51fd-4798-e36f-651633e952e2"
   },
   "outputs": [],
   "source": [
    "# !rm -rf Deep-Learning\n",
    "# !git clone https://github.com/tmengel/Deep-Learning.git\n",
    "# !ls Deep-Learning/project3/\n",
    "# %cd Deep-Learning/project3\n",
    "# !pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7c3f5eb",
   "metadata": {},
   "source": [
    "---\n",
    "The following imports and functions are the data pre-processing and the task functions which load the data, process it, one hot endoce it depending on the target feature and then train and evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3154b4d",
   "metadata": {
    "id": "c3154b4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "age_labels = ['0-2','3-9','10-19','20-29','30-39','40-49','50-59','60-69','more than 70'] # possible values for age\n",
    "gender_labels = ['Female','Male'] # possible values for gender\n",
    "race_labels= ['East Asian', 'Indian', 'Black', 'White', 'Middle Eastern', 'Latino_Hispanic', 'Southeast Asian'] # possile races\n",
    "features = ['age','gender','race'] # all target features\n",
    "cat_labels = {\n",
    "    'age': age_labels,\n",
    "    'gender' : gender_labels,\n",
    "    'race' : race_labels,\n",
    "    # 'service_test' : service_test_labels\n",
    "    }\n",
    "\n",
    "TRAIN = 10 # number of images to use for training\n",
    "VALIDATE = 10 # number of images to use for training and validation\n",
    "\n",
    "# training_path = '/content/Deep-Learning/project3/fairface_label_train.csv'\n",
    "# validation_path = '/content/Deep-Learning/project3/fairface_label_val.csv'\n",
    "training_path = 'fairface_label_train.csv' # path to training data\n",
    "validation_path = 'fairface_label_val.csv' # path to validation data\n",
    "\n",
    "def one_hot_encode(df,feature,labels):\n",
    "    '''\n",
    "    One-hot encode a feature in a dataframe\n",
    "    '''\n",
    "    cats = df[feature].values\n",
    "    one_hot = np.zeros((cats.size,labels.__len__()))\n",
    "    for i in range(cats.size):\n",
    "        one_hot[i][labels.index(cats[i])] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_decode(one_hot,labels):\n",
    "    ''' \n",
    "        Decode one-hot encoded labels to their original labels\n",
    "    '''\n",
    "    return [labels[np.argmax(one_hot[i])] for i in range(one_hot.shape[0])]\n",
    "\n",
    "def read_images(n, path):\n",
    "    ''' \n",
    "        Read in n images from a csv file, and return a dataframe with the labels and the images \n",
    "    '''    \n",
    "    \n",
    "    # read in csv file\n",
    "    df = pd.read_csv(path)\n",
    "    # sample n rows\n",
    "    df = df.sample(n, random_state=42)\n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    # create empty list for images\n",
    "    images = np.zeros((n, 32, 32, 1), dtype=np.uint8)\n",
    "    # loop through each row and read in image file        \n",
    "    for index, row in df.iterrows():\n",
    "#         image_path = f'/content/Deep-Learning/project3/{row[\"file\"]}'\n",
    "        image_path = f'{row[\"file\"]}'\n",
    "        image = Image.open(image_path)\n",
    "        img = np.array(image)\n",
    "        img = img.reshape(32,32,1)\n",
    "        images[index] = img\n",
    "\n",
    "    df = df.drop(columns=[\"file\"])\n",
    "    df['image_id'] = df.index  # add image_id column\n",
    "    return df, images\n",
    "\n",
    "def plot_images(df,images):\n",
    "    '''\n",
    "    Plot images with their labels for visual inspection\n",
    "    '''\n",
    "    # plot images\n",
    "    fig, axs = plt.subplots(len(images)//5, 5, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(df.loc[i, \"age\"] + \", \" + df.loc[i, \"gender\"] + \", \" + df.loc[i, \"race\"])\n",
    "    plt.show()\n",
    "    \n",
    "def min_max_scale_images(images):\n",
    "    '''\n",
    "        Min-max scale images\n",
    "    '''\n",
    "    # Min-max scale each image\n",
    "    images_min_max = np.zeros(images.shape)\n",
    "    iter=0\n",
    "    for i in images:\n",
    "        ama = np.amax(i)\n",
    "        ami = np.amin(i)\n",
    "        i = (i - ami) / (ama - ami)\n",
    "        images_min_max[iter] = i\n",
    "        iter+=1\n",
    "    return images_min_max\n",
    "\n",
    "def count_categories(path):\n",
    "    '''\n",
    "        Count the number of categories for each feature. Used to determine the number of output nodes for the neural network\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    for column in df.columns:\n",
    "        print(f\"{column}: {df[column].nunique()} categories\")\n",
    "\n",
    "def load_data(n, path,plot=False):\n",
    "    '''\n",
    "        Function to load data from a csv file, encode and preprocess the data\n",
    "    '''\n",
    "    \n",
    "    df, images = read_images(n, path) # read in images and labels\n",
    "    images = min_max_scale_images(images) # min-max scale images\n",
    "    if plot: # plot images (for visual inspection)\n",
    "        plot_images(df,images)\n",
    "        \n",
    "    # One-hot encode labels\n",
    "    one_hot = {}\n",
    "    labels = {}\n",
    "    for feature in features:\n",
    "        one_hot[feature] = one_hot_encode(df, feature, cat_labels[feature])\n",
    "        labels[feature] = one_hot_decode(one_hot[feature], cat_labels[feature])\n",
    "    \n",
    "    return images, one_hot, labels\n",
    "\n",
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot training and validation loss and accuracy for task 1-3\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # plot training and validation loss\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[0].set_title('Model Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[1].plot(history.history['accuracy'])\n",
    "    axs[1].plot(history.history['val_accuracy'])\n",
    "    axs[1].set_title('Model Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_model(model, images_test, one_hot_test, labels_test, target):\n",
    "    '''\n",
    "    Evaluate the model on the test data and plot the confusion matrix. Only works for task 1-3\n",
    "    '''\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_acc = model.evaluate(images_test, one_hot_test[target])\n",
    "\n",
    "    # Get predictions for the test data\n",
    "    test_pred_one_hot = model.predict(images_test)\n",
    "    test_pred_labels = one_hot_decode(test_pred_one_hot, cat_labels[target])\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(labels_test[target], test_pred_labels, labels=cat_labels[target])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[target], yticklabels=cat_labels[target])\n",
    "    plt.xlabel('Predicted labels')\n",
    "    ax = plt.gca()\n",
    "    ax.invert_yaxis()\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Final classification accuracy:', test_acc)\n",
    "    \n",
    "def plot_multi_history(history, features):\n",
    "    '''\n",
    "    Plot training and validation loss and accuracy for task 4\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    # plot training and validation loss\n",
    "    axs[0][0].plot(history.history[f'{features[0]}_output_loss'])\n",
    "    axs[0][0].plot(history.history[f'val_{features[0]}_output_loss'])\n",
    "    axs[0][0].set_title(f'{features[0]} Loss')\n",
    "    axs[0][0].set_xlabel('Epoch')\n",
    "    axs[0][0].set_ylabel('Loss')\n",
    "    axs[0][0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[0][1].plot(history.history[f'{features[0]}_output_accuracy'])\n",
    "    axs[0][1].plot(history.history[f'val_{features[0]}_output_accuracy'])\n",
    "    axs[0][1].set_title(f'{features[0]} Accuracy')\n",
    "    axs[0][1].set_xlabel('Epoch')\n",
    "    axs[0][1].set_ylabel('Accuracy')\n",
    "    axs[0][1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    axs[1][0].plot(history.history[f'{features[1]}_output_loss'])\n",
    "    axs[1][0].plot(history.history[f'val_{features[1]}_output_loss'])\n",
    "    axs[1][0].set_title(f'{features[1]} Loss')\n",
    "    axs[1][0].set_xlabel('Epoch')\n",
    "    axs[1][0].set_ylabel('Loss')\n",
    "    axs[1][0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[1][1].plot(history.history[f'{features[1]}_output_accuracy'])\n",
    "    axs[1][1].plot(history.history[f'val_{features[1]}_output_accuracy'])\n",
    "    axs[1][1].set_title(f'{features[1]} Accuracy')\n",
    "    axs[1][1].set_xlabel('Epoch')\n",
    "    axs[1][1].set_ylabel('Accuracy')\n",
    "    axs[1][1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_multi_model(model, images_test, one_hot_test, labels_test, targets):\n",
    "    '''\n",
    "    Evaluate the model on the test data and plot the confusion matrix. Only works for task 4\n",
    "    '''\n",
    "    \n",
    "    evaluation = model.evaluate(images_test, [one_hot_test[targets[0]], one_hot_test[targets[1]]], verbose=0)\n",
    "    print(f'Final classification accuracies: {targets[0]} {evaluation[3]}, {targets[1]} {evaluation[4]}')\n",
    "\n",
    "    test_pred_one_hot_feature1, test_pred_one_hot_feature2  = model.predict(images_test)\n",
    "    test_pred_labels_feature1 = one_hot_decode(test_pred_one_hot_feature1, cat_labels[targets[0]])\n",
    "    test_pred_labels_feature2 = one_hot_decode(test_pred_one_hot_feature2, cat_labels[targets[1]])\n",
    "\n",
    "    cm_feature1 = confusion_matrix(labels_test[targets[0]], test_pred_labels_feature1, labels=cat_labels[targets[0]])\n",
    "    cm_feature2 = confusion_matrix(labels_test[targets[1]], test_pred_labels_feature2, labels=cat_labels[targets[1]])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0] = sns.heatmap(cm_feature1, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[targets[0]], yticklabels=cat_labels[targets[0]], ax=axs[0])\n",
    "    axs[0].set_xlabel(f'Predicted {targets[0]} labels')\n",
    "    axs[0].set_ylabel(f'True {targets[0]} labels')\n",
    "    axs[0].set_title(f'{targets[0]} Confusion matrix')\n",
    "    axs[0].invert_yaxis()\n",
    "    axs[1] = sns.heatmap(cm_feature2, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[targets[1]], yticklabels=cat_labels[targets[1]], ax=axs[1])\n",
    "    axs[1].set_xlabel(f'Predicted {targets[1]} labels')\n",
    "    axs[1].set_ylabel(f'True {targets[1]} labels')\n",
    "    axs[1].set_title(f'{targets[1]} Confusion matrix')\n",
    "    axs[1].invert_yaxis()\n",
    "\n",
    "    # plt.title('Confusion matrices')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e421e07",
   "metadata": {},
   "source": [
    "---\n",
    "The following code includes the tensorflow models for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a96e03",
   "metadata": {
    "id": "d9a96e03"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, concatenate, Dropout, Conv2DTranspose, Lambda, Reshape, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def FullyConnectedNetwork(output_size): # Task 1\n",
    "    return Sequential([\n",
    "            Flatten(input_shape=(32, 32, 1)),\n",
    "            Dense(1024, activation='tanh'),\n",
    "            Dense(512, activation='sigmoid'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(output_size, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "def SmallCNN(output_size): # Task 2\n",
    "    return Sequential([\n",
    "            Conv2D(40,input_shape=(32,32,1),kernel_size=(5,5),strides=1,padding='valid', activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Flatten(),\n",
    "            Dense(100,activation='relu'),\n",
    "            Dense(output_size,activation='softmax')\n",
    "        ])\n",
    "    \n",
    "def CustomCNN(output_size): # Task 3\n",
    "    return Sequential([\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_size,activation='softmax')\n",
    "    ])\n",
    "\n",
    "def sampling(args): # Task 5\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    #Extract mean and log of variance\n",
    "    z_mean, z_log_var = args\n",
    "    #get batch size and length of vector (size of latent space)\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    #Return sampled number (need to raise var to correct power)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "def VariationalAutoEncoder(): # Task 5\n",
    "    latent_dim = 3\n",
    "    intermediate_dim = 100\n",
    "    ## Encoding\n",
    "    inputs = Input(shape=(32,32,1),name='encoder_input')\n",
    "    x0 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\",name='encoder_conv1')(inputs)\n",
    "    m0 = MaxPooling2D((2, 2))(x0)\n",
    "    x1 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\",name='encoder_conv2')(m0)\n",
    "    m1 = MaxPooling2D((2, 2))(x1)\n",
    "    f1 = Flatten()(m1)\n",
    "    x = Dense(intermediate_dim, activation='relu', name=\"encoder_hidden_dense\")(f1)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Lambda(sampling, name='z')([z_mean, z_log_var])\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder_output')\n",
    "#     encoder.summary()\n",
    "    \n",
    "#     ## Decoding\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x2 = Dense(intermediate_dim, activation='relu', name=\"decoder_hidden_dense\")(latent_inputs)\n",
    "    x3 = Dense(32*32, activation='relu', name=\"decoder_hidden_dense2\")(x2)\n",
    "    r2 = Reshape((32,32,1))(x3)\n",
    "#     u2 = UpSampling2D((2,2))(r2)\n",
    "    x4 = Conv2DTranspose(128, (3, 3), padding=\"same\", name='decoder_conv1')(r2)\n",
    "#     u4 = UpSampling2D((2,2))(x4)\n",
    "    x5 = Conv2DTranspose(64, (3, 3), padding=\"same\", name='decoder_conv2')(x4)\n",
    "    outputs = Conv2DTranspose(1, (3, 3), padding=\"same\", name='decoder_conv3')(x5)\n",
    "    decoder = Model(latent_inputs,outputs,name='decoder_output')\n",
    "#     decoder.summary()\n",
    "    \n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    \n",
    "    return Model(inputs, outputs, name='vae')\n",
    "\n",
    "def MultiTaskCNN(output_size, features): # Task 4\n",
    "    # Define inputs\n",
    "    inputs = Input(shape=(32, 32, 1))\n",
    "\n",
    "    # Convolutional layers for feature1 branch\n",
    "    feature1_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',)(inputs)\n",
    "    feature1_pool1 = MaxPooling2D(pool_size=(2, 2))(feature1_conv1)\n",
    "    feature1_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature1_pool1)\n",
    "    feature1_pool2 = MaxPooling2D(pool_size=(2, 2))(feature1_conv2)\n",
    "\n",
    "    # Convolutional layers for feature2 branch\n",
    "    feature2_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    feature2_pool1 = MaxPooling2D(pool_size=(2, 2))(feature2_conv1)\n",
    "    feature2_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature2_pool1)\n",
    "    feature2_pool2 = MaxPooling2D(pool_size=(2, 2))(feature2_conv2)\n",
    "\n",
    "    # Flatten and concatenate branches\n",
    "    feature1_flatten = Flatten()(feature1_pool2)\n",
    "    feature2_flatten = Flatten()(feature2_pool2)\n",
    "    merged = concatenate([feature1_flatten, feature2_flatten])\n",
    "\n",
    "    # Fully connected layers\n",
    "    dense1 = Dense(128, activation='relu')(merged)\n",
    "    feature1_dense2 = Dense(64, activation='relu')(dense1)\n",
    "    feature1_output = Dense(output_size[0], activation='softmax', name=f'{features[0]}_output')(feature1_dense2)\n",
    "    feature2_dense2 = Dense(64, activation='relu')(dense1)\n",
    "    feature2_output = Dense(output_size[1], activation='softmax', name=f'{features[1]}_output')(feature2_dense2)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=[feature1_output, feature2_output])   \n",
    "\n",
    "def do_task(target, model, ntrain, nval, loss, optimizer, epochs, batch_size, metrics): # function to do the task 1-3\n",
    "    '''\n",
    "    Function to do the task 1-3. Create the model, train it, plot the training and validation loss and accuracy, and evaluate the model on the test set.\n",
    "    '''\n",
    "    \n",
    "    images_train, one_hot_train, labels_train = load_data(ntrain, training_path, plot=False)\n",
    "    images_test, one_hot_test, labels_test = load_data(nval, validation_path, plot=False)\n",
    "    \n",
    "    # Create the model   \n",
    "    output_size = len(cat_labels[target])\n",
    "    if model == 'FullyConnectedNetwork':\n",
    "        model = FullyConnectedNetwork(output_size)\n",
    "    elif model == 'SmallCNN':\n",
    "        model = SmallCNN(output_size)\n",
    "    elif model == 'CustomCNN':\n",
    "        model = CustomCNN(output_size)\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(images_train, one_hot_train[target], epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    \n",
    "    # Plot the training and validation loss and accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    # Evaluate the model on test data\n",
    "    evaluate_model(model, images_test, one_hot_test, labels_test, target) \n",
    "\n",
    "def do_multi_task(targets, model, ntrain, nval, loss, optimizer, epochs, batch_size, metrics): # function to do the task 4\n",
    "    '''\n",
    "    Function to do the task 4. Create the model, train it, plot the training and validation loss and accuracy, and evaluate the model on the test set.\n",
    "    '''\n",
    "    \n",
    "    images_train, one_hot_train, labels_train = load_data(ntrain, training_path, plot=False)\n",
    "    images_test, one_hot_test, labels_test = load_data(nval, validation_path, plot=False)\n",
    "\n",
    "    # Create the model   \n",
    "    output_size = [len(cat_labels[target]) for target in targets]\n",
    "    model = MultiTaskCNN(output_size, targets)       \n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss = {f'{targets[0]}_output': loss, f'{targets[1]}_output': loss},\n",
    "                    metrics=metrics)\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(images_train, {f'{targets[0]}_output': one_hot_train[targets[0]], f'{targets[1]}_output': one_hot_train[targets[1]]},\n",
    "        epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    plot_multi_history(history, targets)\n",
    "\n",
    "    evaluate_multi_model(model, images_test, one_hot_test, labels_test, targets) \n",
    "    \n",
    "def do_VAE_task(ntrain,nval,loss,optimizer,epochs,batch_size,metrics): # function to do the task 5\n",
    "    images_train, one_hot_train, labels_train = load_data(ntrain, training_path, plot=False)\n",
    "    images_test, one_hot_test, labels_test = load_data(nval, validation_path, plot=False)\n",
    "    \n",
    "    model = VariationalAutoEncoder()\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12b26f7",
   "metadata": {
    "id": "b12b26f7"
   },
   "source": [
    "---\n",
    "__Preview Dataset.__\n",
    "\n",
    "These are a sample of the images in the dataset. The images are 32x32 grayscale images and min-maxed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b7e69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "de4b7e69",
    "outputId": "c0497cd8-2298-4c22-fbef-46d2cbe9f2a2"
   },
   "outputs": [],
   "source": [
    "images_train, one_hot_train, labels_train = load_data(20, training_path, plot=True) # sample 20 images from the training set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3359c032",
   "metadata": {
    "id": "3359c032"
   },
   "source": [
    "---\n",
    "---\n",
    "__Task 1: Fully Connected Neural Network__\n",
    "\n",
    "This network is a dense squential model which is defined in `FullyConnectedNetwork` function above. It consists of 3 hidden layers with 1024, 512, and 100 neurons. The corresponding activation functions are tanh, sigmoid, and relu. The output of the model is a dense layer with $n$ nodes depending on the number of output categories with a soft-max activation. The model is optimized with the adam optimizer and the loss function is categorical cross entropy. The model is trained for 50 epochs with a mini-batch size of 32. For each of the target features, the model summary, epoch training history, accuracy and loss vs epoch plots, confusion matrix, and final categorical accuracy are shown below. The model is trained on the training set with an 80/20 validation split. The model is then evaluated on the validation csv file set. The results are shown below.\n",
    "\n",
    "For more detailed explaination of the model, the keras sequential model is shown below.\n",
    "\n",
    "        Flatten(input_shape=(32, 32, 1)),\n",
    "        Dense(1024, activation='tanh'),\n",
    "        Dense(512, activation='sigmoid'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(output_size, activation='softmax')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d1dc2c4",
   "metadata": {},
   "source": [
    "*Target 1: Age*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc2fd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "77cc2fd8",
    "outputId": "68432734-9f81-49e8-b6f3-a21653905912"
   },
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"FullyConnectedNetwork\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f990635",
   "metadata": {},
   "source": [
    "*Target 2: Gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee9867",
   "metadata": {
    "id": "1cee9867"
   },
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"FullyConnectedNetwork\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c67bba9",
   "metadata": {
    "id": "5c67bba9"
   },
   "source": [
    "---\n",
    "---\n",
    "__Task 2: Small Convolutional Neural Network__\n",
    "\n",
    "This network is a 2D convolution squential model which is defined in `SmallCNN` function above. It consists of 2 hidden layers. The first is a `Conv2D` layer which has 40 features, a $5x5$ kernal size, stride 1 and padding valid. The activation for this convolution layer is relu. The convolution later is followed by a max pooling layer with $2x2$ pool size. The final hidden layer is a dense layer with 100 neurons and a relu activation function. The output of the model is a dense layer with $n$ nodes depending on the number of output categories with a soft-max activation. The model is optimized with the adam optimizer and the loss function is categorical cross entropy. The model is trained for 50 epochs with a mini-batch size of 32. For each of the target features, the model summary, epoch training history, accuracy and loss vs epoch plots, confusion matrix, and final categorical accuracy are shown below. The model is trained on the training set with an 80/20 validation split. The model is then evaluated on the validation csv file set. The results are shown below.\n",
    "\n",
    "For more detailed explaination of the model, the keras sequential model is shown below.\n",
    "\n",
    "        Conv2D(40,input_shape=(32,32,1),kernel_size=(5,5),strides=1,padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(100,activation='relu'),\n",
    "        Dense(output_size,activation='softmax')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a455c0",
   "metadata": {},
   "source": [
    "*Target 1: Age*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27499bf1",
   "metadata": {
    "id": "27499bf1"
   },
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"SmallCNN\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7dc39a5",
   "metadata": {},
   "source": [
    "*Target 2: Gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a62b5",
   "metadata": {
    "id": "6c0a62b5"
   },
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"SmallCNN\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9398bf59",
   "metadata": {
    "id": "9398bf59"
   },
   "source": [
    "---\n",
    "---\n",
    "__Task 3: Your own Convolutional Neural Network__\n",
    "\n",
    "This network is a 2D convolution squential model which is defined in `CustomCNN` function above. This model is much larger than the small convolutional network in the previous task. The first hidden layer is a `Conv2D` layer which has 64 features, a $3x3$ kernal size, stride 1 and padding same. The activation for this convolution layer is relu. The convolution layer is followed by a max pooling layer with $2x2$ pool size. After the first pooling is another2 `Conv2D` layer with 128 and 256 features, another $2x2$ max pooling after each `2Dconv` layer. The final hidden layer is a dense layer with 512 neurons and a relu activation function. There is a 50% drop out layer after the fully connected layer to avoid over fitting. The output of the model is a dense layer with $n$ nodes depending on the number of output categories with a soft-max activation. The model is optimized with the adam optimizer and the loss function is categorical cross entropy. The model is trained for 50 epochs with a mini-batch size of 32. For each of the target features, the model summary, epoch training history, accuracy and loss vs epoch plots, confusion matrix, and final categorical accuracy are shown below. The model is trained on the training set with an 80/20 validation split. The model is then evaluated on the validation csv file set. The results are shown below.\n",
    "\n",
    "For more detailed explaination of the model, the keras sequential model is shown below.\n",
    "\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_size,activation='softmax')\n",
    "\n",
    "These values where chosen due to their increased performace.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b08aae0",
   "metadata": {},
   "source": [
    "*Target 1: Age*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd9893",
   "metadata": {
    "id": "fabd9893"
   },
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"CustomCNN\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa6c2da",
   "metadata": {},
   "source": [
    "*Target 2: Gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9509a4",
   "metadata": {
    "id": "ac9509a4"
   },
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"CustomCNN\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71428fb7",
   "metadata": {
    "id": "71428fb7"
   },
   "source": [
    "---\n",
    "---\n",
    "__Task 4: Your own Convolutional Neural Network on both Tasks Simultaneously__\n",
    "\n",
    "This network is a 2D convolution model which is defined in `MultiTaskCNN` function above. The output of the model is a dense layer with $n$ nodes depending on the number of output categories with a soft-max activation. The model is optimized with the adam optimizer and the loss function is categorical cross entropy. The model is trained for 100 epochs with a mini-batch size of 32. For each of the target features, the model summary, epoch training history, accuracy and loss vs epoch plots, confusion matrix, and final categorical accuracy are shown below. The model is trained on the training set with an 80/20 validation split. The model is then evaluated on the validation csv file set. The results are shown below.\n",
    "\n",
    "For more detailed explaination of the model, the keras sequential model is shown below.\n",
    "\n",
    "        feature1_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',)(Input(input_shape=(32, 32, 1)))\n",
    "        feature1_pool1 = MaxPooling2D(pool_size=(2, 2))(feature1_conv1)\n",
    "        feature1_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature1_pool1)\n",
    "        feature1_pool2 = MaxPooling2D(pool_size=(2, 2))(feature1_conv2)\n",
    "\n",
    "        feature2_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "        feature2_pool1 = MaxPooling2D(pool_size=(2, 2))(feature2_conv1)\n",
    "        feature2_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature2_pool1)\n",
    "        feature2_pool2 = MaxPooling2D(pool_size=(2, 2))(feature2_conv2)\n",
    "\n",
    "        feature1_flatten = Flatten()(feature1_pool2)\n",
    "        feature2_flatten = Flatten()(feature2_pool2)\n",
    "        merged = concatenate([feature1_flatten, feature2_flatten])\n",
    "\n",
    "        dense1 = Dense(128, activation='relu')(merged)\n",
    "        feature1_dense2 = Dense(64, activation='relu')(dense1)\n",
    "        feature1_output = Dense(output_size[0], activation='softmax', name=f'{features[0]}_output')(feature1_dense2)\n",
    "        feature2_dense2 = Dense(64, activation='relu')(dense1)\n",
    "        feature2_output = Dense(output_size[1], activation='softmax', name=f'{features[1]}_output')(feature2_dense2)\n",
    "\n",
    "These values where chosen due to their increased performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ed6e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "a27ed6e7",
    "outputId": "0a436acf-a554-42ba-b26d-136920086ad6"
   },
   "outputs": [],
   "source": [
    "do_multi_task(targets=['age','gender'], model=\"MultiTaskCNN\", ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=100, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dec4d71e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "__Task 5: Variational Auto Encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_VAE_task(ntrain=TRAIN, nval=VALIDATE, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
