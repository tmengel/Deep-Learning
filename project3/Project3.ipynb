{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9150a5",
   "metadata": {},
   "source": [
    "Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3154b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "age_labels = ['0-2','3-9','10-19','20-29','30-39','40-49','50-59','60-69','more than 70']\n",
    "gender_labels = ['Female','Male']\n",
    "race_labels= ['East Asian', 'Indian', 'Black', 'White', 'Middle Eastern', 'Latino_Hispanic', 'Southeast Asian']\n",
    "service_test_labels = ['True', 'False']\n",
    "features = ['age','gender','race']\n",
    "cat_labels = {\n",
    "    'age': age_labels,\n",
    "    'gender' : gender_labels,\n",
    "    'race' : race_labels,\n",
    "    # 'service_test' : service_test_labels\n",
    "    }\n",
    "\n",
    "training_path = 'fairface_label_train.csv'\n",
    "validation_path = 'fairface_label_val.csv'\n",
    "\n",
    "\n",
    "def one_hot_encode(df,feature,labels):\n",
    "    cats = df[feature].values\n",
    "    one_hot = np.zeros((cats.size,labels.__len__()))\n",
    "    for i in range(cats.size):\n",
    "        one_hot[i][labels.index(cats[i])] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_decode(one_hot,labels):\n",
    "    return [labels[np.argmax(one_hot[i])] for i in range(one_hot.shape[0])]\n",
    "\n",
    "def read_images(n, path):\n",
    "    # read in csv file\n",
    "    df = pd.read_csv(path)\n",
    "    # sample n rows\n",
    "    df = df.sample(n, random_state=42)\n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    # create empty list for images\n",
    "    images = np.zeros((n, 32, 32, 1), dtype=np.uint8)\n",
    "    # loop through each row and read in image file        \n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row[\"file\"]\n",
    "        image = Image.open(image_path)\n",
    "        img = np.array(image)\n",
    "        img = img.reshape(32,32,1)\n",
    "        images[index] = img\n",
    "\n",
    "    df = df.drop(columns=[\"file\"])\n",
    "    df['image_id'] = df.index   \n",
    "    return df, images\n",
    "\n",
    "def plot_images(df,images):\n",
    "    # plot images\n",
    "    fig, axs = plt.subplots(len(images)//5, 5, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(df.loc[i, \"age\"] + \", \" + df.loc[i, \"gender\"] + \", \" + df.loc[i, \"race\"])\n",
    "    plt.show()\n",
    "    \n",
    "def min_max_scale_images(images):\n",
    "    # Min-max scale each image\n",
    "    images_min_max = np.zeros(images.shape)\n",
    "    iter=0\n",
    "    for i in images:\n",
    "        ama = np.amax(i)\n",
    "        ami = np.amin(i)\n",
    "        i = (i - ami) / (ama - ami)\n",
    "        images_min_max[iter] = i\n",
    "        iter+=1\n",
    "    return images_min_max\n",
    "\n",
    "def count_categories(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for column in df.columns:\n",
    "        print(f\"{column}: {df[column].nunique()} categories\")\n",
    "\n",
    "def load_data(n, path,plot=False):\n",
    "    df, images = read_images(n, path)\n",
    "    images = min_max_scale_images(images)\n",
    "    if plot:\n",
    "        plot_images(df,images)\n",
    "    \n",
    "    \n",
    "    # One-hot encode labels\n",
    "    one_hot = {}\n",
    "    labels = {}\n",
    "    for feature in features:\n",
    "        one_hot[feature] = one_hot_encode(df, feature, cat_labels[feature])\n",
    "        labels[feature] = one_hot_decode(one_hot[feature], cat_labels[feature])\n",
    "    \n",
    "    return images, one_hot, labels\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # plot training and validation loss\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[0].set_title('Model Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[1].plot(history.history['accuracy'])\n",
    "    axs[1].plot(history.history['val_accuracy'])\n",
    "    axs[1].set_title('Model Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_model(model, images_test, one_hot_test, labels_test, target):\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_acc = model.evaluate(images_test, one_hot_test[target])\n",
    "\n",
    "    # Get predictions for the test data\n",
    "    test_pred_one_hot = model.predict(images_test)\n",
    "    test_pred_labels = one_hot_decode(test_pred_one_hot, cat_labels[target])\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(labels_test[target], test_pred_labels, labels=cat_labels[target])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[target], yticklabels=cat_labels[target])\n",
    "    plt.xlabel('Predicted labels')\n",
    "    ax = plt.gca()\n",
    "    ax.invert_yaxis()\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Final classification accuracy:', test_acc)\n",
    "    \n",
    "def plot_multi_history(history, features):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    # plot training and validation loss\n",
    "    axs[0][0].plot(history.history[f'{features[0]}_output_loss'])\n",
    "    axs[0][0].plot(history.history[f'val_{features[0]}_output_loss'])\n",
    "    axs[0][0].set_title(f'{features[0]} Loss')\n",
    "    axs[0][0].set_xlabel('Epoch')\n",
    "    axs[0][0].set_ylabel('Loss')\n",
    "    axs[0][0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[0][1].plot(history.history[f'{features[0]}_output_accuracy'])\n",
    "    axs[0][1].plot(history.history[f'val_{features[0]}_output_accuracy'])\n",
    "    axs[0][1].set_title(f'{features[0]} Accuracy')\n",
    "    axs[0][1].set_xlabel('Epoch')\n",
    "    axs[0][1].set_ylabel('Accuracy')\n",
    "    axs[0][1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    axs[1][0].plot(history.history[f'{features[1]}_output_loss'])\n",
    "    axs[1][0].plot(history.history[f'val_{features[1]}_output_loss'])\n",
    "    axs[1][0].set_title(f'{features[1]} Loss')\n",
    "    axs[1][0].set_xlabel('Epoch')\n",
    "    axs[1][0].set_ylabel('Loss')\n",
    "    axs[1][0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    # plot training and validation accuracy\n",
    "    axs[1][1].plot(history.history[f'{features[1]}_output_accuracy'])\n",
    "    axs[1][1].plot(history.history[f'val_{features[1]}_output_accuracy'])\n",
    "    axs[1][1].set_title(f'{features[1]} Accuracy')\n",
    "    axs[1][1].set_xlabel('Epoch')\n",
    "    axs[1][1].set_ylabel('Accuracy')\n",
    "    axs[1][1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_multi_model(model, images_test, one_hot_test, labels_test, targets):\n",
    "    evaluation = model.evaluate(images_test, [one_hot_test[targets[0]], one_hot_test[targets[1]]], verbose=0)\n",
    "    print(f'Final classification accuracies: {targets[0]} {evaluation[3]}, {targets[1]} {evaluation[4]}')\n",
    "\n",
    "    test_pred_one_hot_feature1, test_pred_one_hot_feature2  = model.predict(images_test)\n",
    "    test_pred_labels_feature1 = one_hot_decode(test_pred_one_hot_feature1, cat_labels[targets[0]])\n",
    "    test_pred_labels_feature2 = one_hot_decode(test_pred_one_hot_feature2, cat_labels[targets[1]])\n",
    "\n",
    "    cm_feature1 = confusion_matrix(labels_test[targets[0]], test_pred_labels_feature1, labels=cat_labels[targets[0]])\n",
    "    cm_feature2 = confusion_matrix(labels_test[targets[1]], test_pred_labels_feature2, labels=cat_labels[targets[1]])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0] = sns.heatmap(cm_feature1, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[targets[0]], yticklabels=cat_labels[targets[0]], ax=axs[0])\n",
    "    axs[0].set_xlabel(f'Predicted {targets[0]} labels')\n",
    "    axs[0].set_ylabel(f'True {targets[0]} labels')\n",
    "    axs[0].set_title(f'{targets[0]} Confusion matrix')\n",
    "    axs[0].invert_yaxis()\n",
    "    axs[1] = sns.heatmap(cm_feature2, annot=True, cmap='viridis', fmt='g', xticklabels=cat_labels[targets[1]], yticklabels=cat_labels[targets[1]], ax=axs[1])\n",
    "    axs[1].set_xlabel(f'Predicted {targets[1]} labels')\n",
    "    axs[1].set_ylabel(f'True {targets[1]} labels')\n",
    "    axs[1].set_title(f'{targets[1]} Confusion matrix')\n",
    "    axs[1].invert_yaxis()\n",
    "\n",
    "    # plt.title('Confusion matrices')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a96e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def FullyConnectedNetwork(output_size):\n",
    "    return Sequential([\n",
    "            Flatten(input_shape=(32, 32, 1)),\n",
    "            Dense(1024, activation='tanh'),\n",
    "            Dense(512, activation='sigmoid'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(output_size, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "def SmallCNN(output_size):\n",
    "    return Sequential([\n",
    "            Conv2D(40,input_shape=(32,32,1),kernel_size=(5,5),strides=1,padding='valid', activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Flatten(),\n",
    "            Dense(100,activation='relu'),\n",
    "            Dense(output_size,activation='softmax')\n",
    "        ])\n",
    "    \n",
    "def CustomCNN(output_size):\n",
    "    return Sequential([\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_size,activation='softmax')\n",
    "    ])\n",
    "\n",
    "def MultiTaskCNN(output_size, features):\n",
    "    # Define inputs\n",
    "    inputs = Input(shape=(32, 32, 1))\n",
    "\n",
    "    # Convolutional layers for feature1 branch\n",
    "    feature1_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',)(inputs)\n",
    "    feature1_pool1 = MaxPooling2D(pool_size=(2, 2))(feature1_conv1)\n",
    "    feature1_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature1_pool1)\n",
    "    feature1_pool2 = MaxPooling2D(pool_size=(2, 2))(feature1_conv2)\n",
    "\n",
    "    # Convolutional layers for feature2 branch\n",
    "    feature2_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    feature2_pool1 = MaxPooling2D(pool_size=(2, 2))(feature2_conv1)\n",
    "    feature2_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(feature2_pool1)\n",
    "    feature2_pool2 = MaxPooling2D(pool_size=(2, 2))(feature2_conv2)\n",
    "\n",
    "    # Flatten and concatenate branches\n",
    "    feature1_flatten = Flatten()(feature1_pool2)\n",
    "    feature2_flatten = Flatten()(feature2_pool2)\n",
    "    merged = concatenate([feature1_flatten, feature2_flatten])\n",
    "\n",
    "    # Fully connected layers\n",
    "    dense1 = Dense(128, activation='relu')(merged)\n",
    "    feature1_dense2 = Dense(64, activation='relu')(dense1)\n",
    "    feature1_output = Dense(output_size[0], activation='softmax', name=f'{features[0]}_output')(feature1_dense2)\n",
    "    feature2_dense2 = Dense(64, activation='relu')(dense1)\n",
    "    feature2_output = Dense(output_size[1], activation='softmax', name=f'{features[1]}_output')(feature2_dense2)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=[feature1_output, feature2_output])   \n",
    "\n",
    "def do_task(target, model, ntrain, nval, loss, optimizer, epochs, batch_size, metrics):\n",
    "    \n",
    "    images_train, one_hot_train, labels_train = load_data(ntrain, training_path, plot=False)\n",
    "    images_test, one_hot_test, labels_test = load_data(nval, validation_path, plot=False)\n",
    "    \n",
    "    # Create the model   \n",
    "    output_size = len(cat_labels[target])\n",
    "    if model == 'FullyConnectedNetwork':\n",
    "        model = FullyConnectedNetwork(output_size)\n",
    "    elif model == 'SmallCNN':\n",
    "        model = SmallCNN(output_size)\n",
    "    elif model == 'CustomCNN':\n",
    "        model = CustomCNN(output_size)\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(images_train, one_hot_train[target], epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    \n",
    "    # Plot the training and validation loss and accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    # Evaluate the model on test data\n",
    "    evaluate_model(model, images_test, one_hot_test, labels_test, target) \n",
    "\n",
    "def do_multi_task(targets, model, ntrain, nval, loss, optimizer, epochs, batch_size, metrics):\n",
    "    \n",
    "    images_train, one_hot_train, labels_train = load_data(ntrain, training_path, plot=False)\n",
    "    images_test, one_hot_test, labels_test = load_data(nval, validation_path, plot=False)\n",
    "\n",
    "    # Create the model   \n",
    "    output_size = [len(cat_labels[target]) for target in targets]\n",
    "    model = MultiTaskCNN(output_size, targets)       \n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss = {f'{targets[0]}_output': loss, f'{targets[1]}_output': loss},\n",
    "                    metrics=metrics)\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(images_train, {f'{targets[0]}_output': one_hot_train[targets[0]], f'{targets[1]}_output': one_hot_train[targets[1]]},\n",
    "        epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    plot_multi_history(history, targets)\n",
    "\n",
    "    evaluate_multi_model(model, images_test, one_hot_test, labels_test, targets) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12b26f7",
   "metadata": {},
   "source": [
    "Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, one_hot_train, labels_train = load_data(20, training_path, plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3359c032",
   "metadata": {},
   "source": [
    "Task 1: Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"FullyConnectedNetwork\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"FullyConnectedNetwork\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c67bba9",
   "metadata": {},
   "source": [
    "Task 2: Small Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27499bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"SmallCNN\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"SmallCNN\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9398bf59",
   "metadata": {},
   "source": [
    "Task 3: Your own Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='age', model=\"CustomCNN\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_task(target='gender', model=\"CustomCNN\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=10, batch_size=32, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71428fb7",
   "metadata": {},
   "source": [
    "Task 4: Your own Convolutional Neural Network on both Tasks Simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ed6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_multi_task(targets=['age','gender'], model=\"MultiTaskCNN\", ntrain=100, nval=100, loss='categorical_crossentropy', optimizer='adam', epochs=2, batch_size=32, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
