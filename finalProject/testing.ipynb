{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulsenet as pn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = pn.LoadModel(\"trace_autoencoder_fine_tuned.h5\", \"phase_regressor_fine_tuned.h5\", \"amplitude_regressor_fine_tuned.h5\", \"pileup_classifier_fine_tuned.h5\")\n",
    "model_transfer_learned = pn.LoadModel(\"trace_autoencoder_transfer_learned.h5\", \"phase_regressor_initial.h5\", \"amplitude_regressor_initial.h5\", \"pileup_classifier_initial.h5\")\n",
    "model_initial = pn.LoadModel(\"trace_autoencoder_only_pileup.h5\", \"phase_regressor_initial.h5\", \"amplitude_regressor_initial.h5\", \"pileup_classifier_initial.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ysoTracesNoPileup.root\"\n",
    "x_trace, y_trace, y_phase, y_amp =  pn.CreateMockData(filename, pileup_split=0.5, phase_min=0, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "y_pileup_onehot = pn.OneHotEncodePileup(y_phase)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on data\n",
    "trace_pred_fine_tuned, phase_pred_fine_tuned, amp_pred_fine_tuned, pileup_pred_fine_tuned = model_fine_tuned.predict(x_trace)\n",
    "trace_pred_transfer_learned, phase_pred_transfer_learned, amp_pred_transfer_learned, pileup_pred_transfer_learned = model_transfer_learned.predict(x_trace)\n",
    "trace_pred_initial, phase_pred_initial, amp_pred_initial, pileup_pred_initial = model_initial.predict(x_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "performance_fine_tuned = model_fine_tuned.evaluate(x_trace, [y_trace, y_phase, y_amp, y_pileup_onehot], verbose=0)\n",
    "performance_transfer_learned = model_transfer_learned.evaluate(x_trace, [y_trace, y_phase, y_amp, y_pileup_onehot], verbose=0)\n",
    "performance_initial = model_initial.evaluate(x_trace, [y_trace, y_phase, y_amp, y_pileup_onehot], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate residuals\n",
    "phase_fine_tuned_residuals = phase_pred_fine_tuned - y_phase\n",
    "phase_transfer_learned_residuals = phase_pred_transfer_learned - y_phase\n",
    "phase_initial_residuals = phase_pred_initial - y_phase\n",
    "\n",
    "amp_fine_tuned_residuals = amp_pred_fine_tuned - y_amp\n",
    "amp_transfer_learned_residuals = amp_pred_transfer_learned - y_amp\n",
    "amp_initial_residuals = amp_pred_initial - y_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "pred_class_fine_tuned = pileup_pred_fine_tuned[:,0] # pileup classifier output is (1,0) if pileup, (0,1) if not pileup\n",
    "pred_class_transfer_learned = pileup_pred_transfer_learned[:,0]\n",
    "pred_class_initial = pileup_pred_initial[:,0]\n",
    "truth_class = y_pileup_onehot[:,0] # one hot encoded truth is (1,0) if pileup, (0,1) if not pileup\n",
    "\n",
    "ft_fpr, ft_tpr, ft_thresholds = roc_curve(truth_class, pred_class_fine_tuned)\n",
    "tl_fpr, tl_tpr, tl_thresholds = roc_curve(truth_class, pred_class_transfer_learned)\n",
    "in_fpr, in_tpr, in_thresholds = roc_curve(truth_class, pred_class_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ft_fpr, ft_tpr, label=\"Fine Tuned\")\n",
    "plt.plot(tl_fpr, tl_tpr, label=\"Transfer Learned\")\n",
    "plt.plot(in_fpr, in_tpr, label=\"Initial\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label=\"No Skill\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "phase_bins = np.linspace(0,20,20)\n",
    "amp_bins = np.linspace(0.5,1.5,20)\n",
    "\n",
    "pdf = pd.DataFrame({\"Phase\":y_phase[:,0], \"Phase Fine Tuned\":phase_pred_fine_tuned[:,0], \"Phase Transfer Learned\":phase_pred_transfer_learned[:,0], \"Phase Initial\":phase_pred_initial[:,0]})\n",
    "adf = pd.DataFrame({\"Amplitude\":y_amp[:,0], \"Amplitude Fine Tuned\":amp_pred_fine_tuned[:,0], \"Amplitude Transfer Learned\":amp_pred_transfer_learned[:,0], \"Amplitude Initial\":amp_pred_initial[:,0]})\n",
    "\n",
    "p_ft_sigma = []\n",
    "p_tl_sigma = []\n",
    "p_in_sigma = []\n",
    "a_ft_sigma = []\n",
    "a_tl_sigma = []\n",
    "a_in_sigma = []\n",
    "\n",
    "\n",
    "for i in range(0,19):\n",
    "    pdf_bin = pdf[(pdf[\"Phase\"] > phase_bins[i]) & (pdf[\"Phase\"] < phase_bins[i+1])].copy()\n",
    "    adf_bin = adf[(adf[\"Amplitude\"] > amp_bins[i]) & (adf[\"Amplitude\"] < amp_bins[i+1])].copy()\n",
    "    p_ft_sigma.append(pdf_bin[\"Phase Fine Tuned\"].std())\n",
    "    p_tl_sigma.append(pdf_bin[\"Phase Transfer Learned\"].std())\n",
    "    p_in_sigma.append(pdf_bin[\"Phase Initial\"].std())\n",
    "    a_ft_sigma.append(adf_bin[\"Amplitude Fine Tuned\"].std())\n",
    "    a_tl_sigma.append(adf_bin[\"Amplitude Transfer Learned\"].std())\n",
    "    a_in_sigma.append(adf_bin[\"Amplitude Initial\"].std())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(phase_bins[:-1], p_ft_sigma, label=\"Fine Tuned\")\n",
    "plt.plot(phase_bins[:-1], p_tl_sigma, label=\"Transfer Learned\")\n",
    "plt.plot(phase_bins[:-1], p_in_sigma, label=\"Initial\")\n",
    "plt.xlabel(r\"Phase $\\phi$ (ns)\")\n",
    "plt.ylabel(r\"$\\sigma(\\delta\\phi)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(amp_bins[:-1], a_ft_sigma, label=\"Fine Tuned\")\n",
    "plt.plot(amp_bins[:-1], a_tl_sigma, label=\"Transfer Learned\")\n",
    "plt.plot(amp_bins[:-1], a_in_sigma, label=\"Initial\")\n",
    "plt.xlabel(r\"Amplitude $A$ (mV)\")\n",
    "plt.ylabel(r\"$\\sigma(\\delta A)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "ft_kl0 = KL_divergence(trace_pred_fine_tuned[:,0], y_trace[:,0])\n",
    "tl_kl0= KL_divergence(trace_pred_transfer_learned[:,0], y_trace[:,0])   \n",
    "in_kl0 = KL_divergence(trace_pred_initial[:,0], y_trace[:,0])\n",
    "\n",
    "ft_kl1 = KL_divergence(phase_pred_fine_tuned[:,0], y_phase[:,0])\n",
    "tl_kl1 = KL_divergence(phase_pred_transfer_learned[:,0], y_phase[:,0])\n",
    "in_kl1 = KL_divergence(phase_pred_initial[:,0], y_phase[:,0])\n",
    "\n",
    "ft_kl = np.concatenate((ft_kl0, ft_kl1), axis=0)\n",
    "tl_kl = np.concatenate((tl_kl0, tl_kl1), axis=0)\n",
    "in_kl = np.concatenate((in_kl0, in_kl1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ft_kl, bins=100, label=\"Fine Tuned\", alpha=0.5, density=True)\n",
    "plt.hist(tl_kl, bins=100, label=\"Transfer Learned\", alpha=0.5, density=True)\n",
    "plt.hist(in_kl, bins=100, label=\"Initial\", alpha=0.5, density=True)\n",
    "plt.xlabel(\"KL Divergence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
