{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 19:03:58.487829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 19:03:59.790950: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-10 19:03:59.791060: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-10 19:04:02.721824: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-10 19:04:02.722169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-10 19:04:02.722183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-10 19:04:04.982643: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-10 19:04:04.982819: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-10 19:04:04.982904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tmengel): /proc/driver/nvidia/version does not exist\n",
      "2023-05-10 19:04:04.984933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tmengel/Deep-Learning.git\n",
    "# %cd Deep-Learning/finalProject\n",
    "# %pip install uproot\n",
    "# %pip install pandas\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import pulsenet as pn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ysoTracesWithPileup.root\"\n",
    "# filename = \"/content/drive/MyDrive/DeepLearningFinalProject/ysoTracesWithPileup.root\"\n",
    "\n",
    "discriminator_head_weights = \"weights/discriminator_head_initial.h5\"\n",
    "discriminator_base_weights = \"weights/discriminator_base_initial.h5\"\n",
    "classifier_head_weights = \"weights/classifier_head_initial.h5\"\n",
    "classifier_base_weights = \"weights/classifier_base_initial.h5\"\n",
    "phase_weights = \"weights/phase_initial.h5\"\n",
    "amplitude_weights = \"weights/amplitude_initial.h5\"\n",
    "transfer_head_weights = \"weights/transfer_head_initial.h5\"\n",
    "\n",
    "discriminator_base_fine_tuned_weights = \"weights/discriminator_base_fine_tuned.h5\"\n",
    "classifier_base_fine_tuned_weights = \"weights/classifier_base_fine_tuned.h5\"\n",
    "phase_fine_tuned_weights = \"weights/phase_fine_tuned.h5\"\n",
    "amplitude_fine_tuned_weights = \"weights/amplitude_fine_tuned.h5\"\n",
    "transfer_head_fine_tuned_weights = \"weights/transfer_head_fine_tuned.h5\"\n",
    "\n",
    "discriminator_initial_history =\"history/discriminator_initial_history.h5\"\n",
    "classifier_initial_history = \"history/classifier_initial_history.h5\"\n",
    "phase_initial_history = \"history/phase_initial_history.h5\"\n",
    "amplitude_initial_history = \"history/amplitude_initial_history.h5\"\n",
    "\n",
    "discriminator_base_control_weights = \"weights/discriminator_base_control.h5\"\n",
    "classifier_base_control_weights = \"weights/classifier_base_control.h5\"\n",
    "phase_control_weights = \"weights/phase_control.h5\"\n",
    "amplitude_control_weights = \"weights/amplitude_control.h5\"\n",
    "transfer_head_control_weights = \"weights/transfer_head_control.h5\"\n",
    "\n",
    "transfer_history = \"history/transfer_history.h5\"\n",
    "fine_tune_history = \"history/fine_tune_history.h5\"\n",
    "control_history = \"history/control_history.h5\"\n",
    "\n",
    "transfer_model = \"weights/transfer_model.h5\"\n",
    "fine_tuned_model = \"weights/fine_tuned_model.h5\"\n",
    "control_model = \"weights/control_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 261852 samples: 100.0 % pileup, 0.0 % no pileup\n"
     ]
    }
   ],
   "source": [
    "# Train Discriminator\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=1.0, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# output files\n",
    "historyfile = \"history/discriminator_initial_history.h5\"\n",
    "\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceDiscriminatorBase(name=\"discriminator_base\"),\n",
    "    pn.TraceDiscriminatorHead(name=\"discriminator_head\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_trace, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(discriminator_base_weights)\n",
    "model.layers[1].save_weights(discriminator_head_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(discriminator_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "y_pileup = pn.EncodePileup(y_phase)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceClassifierBase(name=\"classifier_base\"),\n",
    "    pn.TraceClassifierHead(name=\"classifier_head\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_pileup, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(classifier_base_weights)\n",
    "model.layers[1].save_weights(classifier_head_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(classifier_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Amplitude Regressor\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceAmplitudeRegressor(name = \"amplitude_regressor\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_amp, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(amplitude_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(amplitude_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase Regressor\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TracePhaseRegressor(name = \"phase_regressor\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_phase, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(phase_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(phase_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(1, 300))  # Returns a placeholder tensor\n",
    "classifer_feature_vec = pn.TraceClassifierBase(name = \"classifier_base\")(input)\n",
    "discriminator_feature_vec = pn.TraceDiscriminatorBase(name = \"discriminator_base\")(input)\n",
    "trace_output = pn.TraceClassifierDiscriminatorHead(name = \"transfer_head\")([discriminator_feature_vec, classifer_feature_vec])\n",
    "phase_output = pn.TracePhaseRegressor(name=\"phase_regressor\")(trace_output)\n",
    "amplitude_output = pn.TraceAmplitudeRegressor(name =\"amplitude_regressor\")(trace_output)\n",
    "classifier_output = pn.TraceClassifierHead(name=\"classifier_head\")(classifer_feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1, 300)]     0           []                               \n",
      "                                                                                                  \n",
      " discriminator_base (TraceDiscr  (None, 2, 300)      992358      ['input_1[0][0]']                \n",
      " iminatorBase)                                                                                    \n",
      "                                                                                                  \n",
      " classifier_base (TraceClassifi  (None, 1, 300)      244456      ['input_1[0][0]']                \n",
      " erBase)                                                                                          \n",
      "                                                                                                  \n",
      " transfer_head (TraceClassifier  (None, 2, 300)      541800      ['discriminator_base[0][0]',     \n",
      " DiscriminatorHead)                                               'classifier_base[0][0]']        \n",
      "                                                                                                  \n",
      " amplitude_regressor (TraceAmpl  (None, 1)           276149      ['transfer_head[0][0]']          \n",
      " itudeRegressor)                                                                                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,054,763\n",
      "Trainable params: 2,054,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "['input_1', 'discriminator_base', 'classifier_base', 'transfer_head', 'amplitude_regressor']\n"
     ]
    }
   ],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer='adam', loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "# freeze layers\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"classifier_base\")].trainable = False\n",
    "model.layers[model_names.index(\"phase_regressor\")].trainable = False\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].trainable = False                           \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_weights)\n",
    "# save model\n",
    "model.save(transfer_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(transfer_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].load_weights(transfer_head_weights)                     \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(discriminator_base_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].save_weights(classifier_base_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(phase_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(amplitude_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_fine_tuned_weights)\n",
    "# save model\n",
    "model.save(fine_tuned_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(fine_tune_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# print summary\n",
    "model.summary()\n",
    "# train control model\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=400, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(discriminator_base_control_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].save_weights(classifier_base_control_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(phase_control_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(amplitude_control_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_control_weights)\n",
    "# save model\n",
    "model.save(control_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(control_history, key=\"hist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
