{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 00:01:17.464122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 00:01:18.851073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-11 00:01:18.851334: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-11 00:01:20.646150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-11 00:01:20.646822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-11 00:01:20.646855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tmengel/Deep-Learning.git\n",
    "# %cd Deep-Learning/finalProject\n",
    "# %pip install uproot\n",
    "# %pip install pandas\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import pulsenet as pn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ysoTracesWithPileup.root\"\n",
    "# filename = \"/content/drive/MyDrive/DeepLearningFinalProject/ysoTracesWithPileup.root\"\n",
    "\n",
    "discriminator_head_weights = \"weights/discriminator_head_initial.h5\"\n",
    "discriminator_base_weights = \"weights/discriminator_base_initial.h5\"\n",
    "classifier_head_weights = \"weights/classifier_head_initial.h5\"\n",
    "classifier_base_weights = \"weights/classifier_base_initial.h5\"\n",
    "phase_weights = \"weights/phase_initial.h5\"\n",
    "amplitude_weights = \"weights/amplitude_initial.h5\"\n",
    "transfer_head_weights = \"weights/transfer_head_initial.h5\"\n",
    "\n",
    "discriminator_base_fine_tuned_weights = \"weights/discriminator_base_fine_tuned.h5\"\n",
    "classifier_base_fine_tuned_weights = \"weights/classifier_base_fine_tuned.h5\"\n",
    "phase_fine_tuned_weights = \"weights/phase_fine_tuned.h5\"\n",
    "amplitude_fine_tuned_weights = \"weights/amplitude_fine_tuned.h5\"\n",
    "transfer_head_fine_tuned_weights = \"weights/transfer_head_fine_tuned.h5\"\n",
    "\n",
    "discriminator_initial_history =\"history/discriminator_initial_history.h5\"\n",
    "classifier_initial_history = \"history/classifier_initial_history.h5\"\n",
    "phase_initial_history = \"history/phase_initial_history.h5\"\n",
    "amplitude_initial_history = \"history/amplitude_initial_history.h5\"\n",
    "\n",
    "discriminator_base_control_weights = \"weights/discriminator_base_control.h5\"\n",
    "classifier_base_control_weights = \"weights/classifier_base_control.h5\"\n",
    "phase_control_weights = \"weights/phase_control.h5\"\n",
    "amplitude_control_weights = \"weights/amplitude_control.h5\"\n",
    "transfer_head_control_weights = \"weights/transfer_head_control.h5\"\n",
    "\n",
    "transfer_history = \"history/transfer_history.h5\"\n",
    "fine_tune_history = \"history/fine_tune_history.h5\"\n",
    "control_history = \"history/control_history.h5\"\n",
    "\n",
    "transfer_model = \"weights/transfer_model.h5\"\n",
    "fine_tuned_model = \"weights/fine_tuned_model.h5\"\n",
    "control_model = \"weights/control_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 261852 samples: 50.17108901211371 % pileup, 49.82891098788629 % no pileup\n",
      "Epoch 1/10\n",
      "450/819 [===============>..............] - ETA: 39s - loss: 0.0041 - accuracy: 0.1582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_trace, y_trace, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Discriminator\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# output files\n",
    "historyfile = \"history/discriminator_initial_history.h5\"\n",
    "\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceDiscriminatorBase(name=\"discriminator_base\"),\n",
    "    pn.TraceDiscriminatorHead(name=\"discriminator_head\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_trace, epochs=10, batch_size=256, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(discriminator_base_weights)\n",
    "model.layers[1].save_weights(discriminator_head_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(discriminator_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "y_pileup = pn.EncodePileup(y_phase)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceClassifierBase(name=\"classifier_base\"),\n",
    "    pn.TraceClassifierHead(name=\"classifier_head\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_pileup, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(classifier_base_weights)\n",
    "model.layers[1].save_weights(classifier_head_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(classifier_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Amplitude Regressor\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TraceAmplitudeRegressor(name = \"amplitude_regressor\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_amp, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(amplitude_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(amplitude_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase Regressor\n",
    "# Get Data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    pn.TracePhaseRegressor(name = \"phase_regressor\")\n",
    "])\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_trace, y_phase, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "model.layers[0].save_weights(phase_weights)\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(phase_initial_history, key=\"hist\")\n",
    "\n",
    "!cp -rf weights /content/drive/MyDrive/DeepLearningFinalProject/\n",
    "!cp -rf history /content/drive/MyDrive/DeepLearningFinalProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(1, 300))  # Returns a placeholder tensor\n",
    "classifer_feature_vec = pn.TraceClassifierBase(name = \"classifier_base\")(input)\n",
    "discriminator_feature_vec = pn.TraceDiscriminatorBase(name = \"discriminator_base\")(input)\n",
    "trace_output = pn.TraceClassifierDiscriminatorHead(name = \"transfer_head\")([discriminator_feature_vec, classifer_feature_vec])\n",
    "phase_output = pn.TracePhaseRegressor(name=\"phase_regressor\")(trace_output)\n",
    "amplitude_output = pn.TraceAmplitudeRegressor(name =\"amplitude_regressor\")(trace_output)\n",
    "classifier_output = pn.TraceClassifierHead(name=\"classifier_head\")(classifer_feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1, 300)]     0           []                               \n",
      "                                                                                                  \n",
      " discriminator_base (TraceDiscr  (None, 2, 300)      992358      ['input_1[0][0]']                \n",
      " iminatorBase)                                                                                    \n",
      "                                                                                                  \n",
      " classifier_base (TraceClassifi  (None, 1, 300)      244456      ['input_1[0][0]']                \n",
      " erBase)                                                                                          \n",
      "                                                                                                  \n",
      " transfer_head (TraceClassifier  (None, 2, 300)      541800      ['discriminator_base[0][0]',     \n",
      " DiscriminatorHead)                                               'classifier_base[0][0]']        \n",
      "                                                                                                  \n",
      " amplitude_regressor (TraceAmpl  (None, 1)           276149      ['transfer_head[0][0]']          \n",
      " itudeRegressor)                                                                                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,054,763\n",
      "Trainable params: 2,054,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "['input_1', 'discriminator_base', 'classifier_base', 'transfer_head', 'amplitude_regressor']\n"
     ]
    }
   ],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer='adam', loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "# freeze layers\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"classifier_base\")].trainable = False\n",
    "model.layers[model_names.index(\"phase_regressor\")].trainable = False\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].trainable = False                           \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_weights)\n",
    "# save model\n",
    "model.save(transfer_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(transfer_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].load_weights(transfer_head_weights)                     \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(discriminator_base_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].save_weights(classifier_base_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(phase_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(amplitude_fine_tuned_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_fine_tuned_weights)\n",
    "# save model\n",
    "model.save(fine_tuned_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(fine_tune_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# print summary\n",
    "model.summary()\n",
    "# train control model\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=400, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(discriminator_base_control_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].save_weights(classifier_base_control_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(phase_control_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(amplitude_control_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_control_weights)\n",
    "# save model\n",
    "model.save(control_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(control_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_head_weights_trace =\"weights/transfer_head_trace.h5\"\n",
    "transfer_history_trace = \"history/transfer_history_trace.h5\"\n",
    "\n",
    "transfer_head_long_transfer_weights = \"weights/transfer_head_long_tranfer.h5\"\n",
    "transfer_history_long_transfer = \"history/transfer_history_long_transfer.h5\"\n",
    "transfer_model_long_transfer = \"weights/transfer_model_long_transfer.h5\"\n",
    "\n",
    "discriminator_base_fine_tuned_weights_long_transfer = \"weights/discriminator_base_fine_tuned_weights_long_transfer.h5\"\n",
    "classifier_base_fine_tuned_weights_long_transfer = \"weights/classifier_base_fine_tuned_weights_long_transfer.h5\"\n",
    "phase_regressor_fine_tuned_weights_long_transfer = \"weights/phase_regressor_fine_tuned_weights_long_transfer.h5\"\n",
    "amplitude_regressor_fine_tuned_weights_long_transfer = \"weights/amplitude_regressor_fine_tuned_weights_long_transfer.h5\"\n",
    "transfer_head_fine_tuned_weights_long_transfer = \"weights/transfer_head_fine_tuned_weights_long_transfer.h5\"\n",
    "\n",
    "fine_tuned_long_transfer_model = \"weights/fine_tuned_long_transfer_model.h5\"\n",
    "fine_tuned_long_transfer_history = \"history/fine_tuned_long_transfer_history.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=trace_output)\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "# freeze layers\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"classifier_base\")].trainable = False                         \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace,y_trace, epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_weights_trace)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(transfer_history_trace, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[phase_output, amplitude_output])\n",
    "model.compile(optimizer='adam', loss=['mse', 'mse'], metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].load_weights(transfer_head_weights_trace)\n",
    "# freeze layers\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"classifier_base\")].trainable = False\n",
    "model.layers[model_names.index(\"phase_regressor\")].trainable = False\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].trainable = False                           \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_long_transfer_weights)\n",
    "# save model\n",
    "model.save(transfer_model_long_transfer)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(transfer_history_long_transfer, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=['mse', 'mse', 'mse'], metrics=['accuracy'])\n",
    "# load weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(discriminator_base_weights)\n",
    "model.layers[model_names.index(\"classifier_base\")].load_weights(classifier_base_weights)\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(phase_weights)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(amplitude_weights)\n",
    "model.layers[model_names.index(\"transfer_head\")].load_weights(transfer_head_long_transfer_weights)                      \n",
    "# print summary\n",
    "model.summary()                                                         \n",
    "# Train Discriminator Head\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=200, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"transfer_head\")].save_weights(transfer_head_fine_tuned_weights_long_transfer)\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(discriminator_base_fine_tuned_weights_long_transfer)\n",
    "model.layers[model_names.index(\"classifier_base\")].save_weights(classifier_base_fine_tuned_weights_long_transfer)\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(phase_regressor_fine_tuned_weights_long_transfer)\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(amplitude_regressor_fine_tuned_weights_long_transfer)\n",
    "\n",
    "# save model\n",
    "model.save(fine_tuned_long_transfer_model)\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(fine_tuned_long_transfer_history, key=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 00:02:14.787707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tmengel/root/lib\n",
      "2023-05-11 00:02:14.787950: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-11 00:02:14.788069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tmengel): /proc/driver/nvidia/version does not exist\n",
      "2023-05-11 00:02:14.790489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 261852 samples: 50.216534530956416 % pileup, 49.783465469043584 % no pileup\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 300)]          0         \n",
      "                                                                 \n",
      " discriminator_base (TraceDi  (None, 2, 300)           992358    \n",
      " scriminatorBase)                                                \n",
      "                                                                 \n",
      " discriminator_head (TraceDi  (None, 2, 300)           90450     \n",
      " scriminatorHead)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,082,808\n",
      "Trainable params: 1,082,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1637/1637 [==============================] - 177s 102ms/step - loss: 0.0031 - accuracy: 0.1951 - val_loss: 0.0020 - val_accuracy: 0.2193\n",
      "Epoch 2/500\n",
      " 229/1637 [===>..........................] - ETA: 2:10 - loss: 0.0025 - accuracy: 0.2185"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_trace, y_trace, epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# save weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tmengel/DeepLearning/Deep-Learning/finalProject/training.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39mlayers[model_names\u001b[39m.\u001b[39mindex(\u001b[39m\"\u001b[39m\u001b[39mdiscriminator_base\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39msave_weights(\u001b[39m\"\u001b[39m\u001b[39mweights/sequential_training_discriminator_base_weights.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1641\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1639\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1640\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1641\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   1642\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m         ):\n\u001b[1;32m   1649\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:1371\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m   1372\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[1;32m   1373\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1374\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[1;32m   1376\u001b[0m )\n\u001b[1;32m   1378\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:638\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 638\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39;49mexecuting_eagerly():\n\u001b[1;32m    639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    640\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py:2207\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[39mif\u001b[39;00m ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2205\u001b[0m   \u001b[39mreturn\u001b[39;00m default_execution_mode \u001b[39m==\u001b[39m EAGER_MODE\n\u001b[0;32m-> 2207\u001b[0m \u001b[39mreturn\u001b[39;00m ctx\u001b[39m.\u001b[39;49mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py:952\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecuting_eagerly\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    951\u001b[0m   \u001b[39m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_thread_local_data\u001b[39m.\u001b[39;49mis_eager\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load new data\n",
    "input = layers.Input(shape=(1, 300))  # Returns a placeholder tensor\n",
    "discriminator_feature_vec = pn.TraceDiscriminatorBase(name = \"discriminator_base\")(input)\n",
    "trace_output = pn.TraceDiscriminatorHead(name = \"discriminator_head\")(discriminator_feature_vec)\n",
    "phase_output = pn.TracePhaseRegressor(name=\"phase_regressor\")(trace_output)\n",
    "amplitude_output = pn.TraceAmplitudeRegressor(name =\"amplitude_regressor\")(trace_output)\n",
    "\n",
    "x_trace, y_trace, y_phase, y_amp = pn.CreateData(filename, pileup_split=0.5, phase_min=0.1, phase_max=20, amplitude_min=0.5, amplitude_max=1.5)\n",
    "\n",
    "# Train Discriminator\n",
    "# create model\n",
    "model = models.Model(inputs=input, outputs=trace_output)\n",
    "model.compile(optimizer=\"adam\", loss='mse', metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "# print summary\n",
    "model.summary()\n",
    "# train model\n",
    "history = model.fit(x_trace, y_trace, epochs=500, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(\"weights/sequential_training_discriminator_base_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_head\")].save_weights(\"weights/sequential_training_discriminator_head_weights.h5\")\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(\"history/sequential_training_discriminator_history.h5\", key=\"hist\")\n",
    "\n",
    "# Train Phase Regressor\n",
    "model = models.Model(inputs=input, outputs=phase_output)\n",
    "model.compile(optimizer=\"adam\", loss='mse', metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(\"weights/sequential_training_discriminator_base_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_head\")].load_weights(\"weights/sequential_training_discriminator_head_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"discriminator_head\")].trainable = False\n",
    "# print summary\n",
    "model.summary()\n",
    "history = model.fit(x_trace, y_phase, epochs=500, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(\"weights/sequential_training_phase_regressor_weights.h5\")\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(\"history/sequential_training_phase_history.h5\", key=\"hist\")\n",
    "\n",
    "# Train Amplitude Regressor\n",
    "model = models.Model(inputs=input, outputs=amplitude_output)\n",
    "model.compile(optimizer=\"adam\", loss='mse', metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(\"weights/sequential_training_discriminator_base_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_head\")].load_weights(\"weights/sequential_training_discriminator_head_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_base\")].trainable = False\n",
    "model.layers[model_names.index(\"discriminator_head\")].trainable = False\n",
    "# print summary\n",
    "model.summary()\n",
    "history = model.fit(x_trace, y_amp, epochs=500, batch_size=128, validation_split=0.2, verbose=1)\n",
    "# save weights\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(\"weights/sequential_training_amplitude_regressor_weights.h5\")\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(\"history/sequential_training_amplitude_history.h5\", key=\"hist\")\n",
    "\n",
    "#fine tune model\n",
    "model = models.Model(inputs=input, outputs=[trace_output, phase_output, amplitude_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\n",
    "model_names = [model.layers[i].name for i in range(len(model.layers))]\n",
    "model.layers[model_names.index(\"discriminator_base\")].load_weights(\"weights/sequential_training_discriminator_base_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_head\")].load_weights(\"weights/sequential_training_discriminator_head_weights.h5\")\n",
    "model.layers[model_names.index(\"phase_regressor\")].load_weights(\"weights/sequential_training_phase_regressor_weights.h5\")\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].load_weights(\"weights/sequential_training_amplitude_regressor_weights.h5\")\n",
    "\n",
    "#save un tuned model\n",
    "model.save(\"models/sequential_training_model.h5\")\n",
    "\n",
    "# print summary\n",
    "model.summary()\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_trace, [y_trace, y_phase, y_amp], epochs=500, batch_size=128, validation_split=0.2, verbose=1)\n",
    "\n",
    "# save weights\n",
    "model.layers[model_names.index(\"discriminator_base\")].save_weights(\"weights/sequential_fine_tuned_discriminator_base_weights.h5\")\n",
    "model.layers[model_names.index(\"discriminator_head\")].save_weights(\"weights/sequential_fine_tuned_discriminator_head_weights.h5\")\n",
    "model.layers[model_names.index(\"phase_regressor\")].save_weights(\"weights/sequential_fine_tuned_phase_regressor_weights.h5\")\n",
    "model.layers[model_names.index(\"amplitude_regressor\")].save_weights(\"weights/sequential_fine_tuned_amplitude_regressor_weights.h5\")\n",
    "\n",
    "# save history\n",
    "pd.DataFrame(history.history, index=history.epoch, columns=history.history.keys()).to_hdf(\"history/sequential_fine_tuned_history.h5\", key=\"hist\")\n",
    "\n",
    "#save tuned model\n",
    "model.save(\"models/sequential_fine_tuned_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
